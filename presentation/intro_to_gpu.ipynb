{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"TORCHINDUCTOR_DEBUG\"] = \"1\"\n",
    "# os.environ[\"TORCH_LOGS\"] = \"output_code\"\n",
    "\n",
    "import torch\n",
    "from pythia.seq.v2.perf.presentation.kernel import op_triton\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# torch\n",
    "def op_torch(x, w, scale, bias):\n",
    "    y = (x * w[None, :, :]).sum(dim=1)  # (N, D)\n",
    "    z = torch.relu(y * scale[None, :] + bias[None, :])  # (N, D)\n",
    "    return x + z[:, None, :]  # (N, K, D)\n",
    "\n",
    "def op_torch2(x, w, scale, bias):\n",
    "    # Question: Will this speed up? Is this kernel fusion?\n",
    "    return x + torch.relu(\n",
    "        (x * w[None, :, :]).sum(dim=1)\n",
    "        * scale[None, :] + bias[None, :]\n",
    "    )[:, None, :]  # (N, K, D)\n",
    "\n",
    "def op_torch3(x, w, scale, bias):\n",
    "    y = torch.einsum(\"nkd, kd -> nd\", x, w)  # (N, D)\n",
    "    z = torch.relu(y * scale[None, :] + bias[None, :])  # (N, D)\n",
    "    return x + z[:, None, :]  # (N, K, D)\n",
    "\n",
    "# torch compile\n",
    "op_torch_compile = torch.compile(op_torch)\n",
    "\n",
    "def get_inputs(N=100_000, K=32, D=128, requires_grad=False):\n",
    "    # Seed\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Representation\n",
    "    x = torch.randn(N, K, D, device=\"cuda\", requires_grad=requires_grad)\n",
    "\n",
    "    # Learnable weights, scale, bias\n",
    "    w = torch.randn(K, D, device=\"cuda\", requires_grad=requires_grad)\n",
    "    scale = torch.randn(D, device=\"cuda\", requires_grad=requires_grad)\n",
    "    bias = torch.randn(D, device=\"cuda\", requires_grad=requires_grad)\n",
    "\n",
    "    return {\"x\": x, \"w\": w, \"scale\": scale, \"bias\": bias}\n",
    "\n",
    "inputs = get_inputs(N=100_000, requires_grad=True)\n",
    "\n",
    "# torch v torch2\n",
    "diff = (op_torch(**inputs) - op_torch3(**inputs)).abs()\n",
    "print(f\"{'torch v torch2':<10} | diff mean: {diff.mean():.2e} | diff mean: {diff.max():.2e}\")\n",
    "\n",
    "# torch v triton\n",
    "diff = (op_torch(**inputs) - op_triton(**inputs)).abs()\n",
    "print(f\"{'torch v triton':<10} | diff mean: {diff.mean():.2e} | diff mean: {diff.max():.2e}\")\n",
    "# op_torch_compile(**inputs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import partial\n",
    "\n",
    "from torch.profiler import ProfilerActivity\n",
    "from torch.profiler.profiler import profile, schedule\n",
    "\n",
    "from pythia.seq.v2.perf.utils import profiler_to_dataframe, stats_fn\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "inputs = get_inputs(N=100_000)\n",
    "\n",
    "# Define function variants\n",
    "fns = {\n",
    "    \"op_torch\": partial(op_torch, **inputs),\n",
    "    # \"op_torch2\": partial(op_torch2, **inputs),\n",
    "    # \"op_torch3\": partial(op_torch3, **inputs),\n",
    "    # \"op_torch_compile\": partial(op_torch_compile, **inputs),\n",
    "    \"op_triton\": partial(op_triton, **inputs),\n",
    "}\n",
    "\n",
    "ref = fns[\"op_torch\"]().clone()\n",
    "n_repeat = 100\n",
    "for name, fn in fns.items():\n",
    "    stats_fn(fn, inputs, None, label=name, n_warmup=10, n_repeat=100); continue\n",
    "    for _ in range(5):\n",
    "        fn()\n",
    "\n",
    "    with profile(\n",
    "        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "        record_shapes=False,\n",
    "        with_stack=False,\n",
    "        profile_memory=False,\n",
    "    ) as prof:\n",
    "        for _ in range(n_repeat):\n",
    "            out = fn()\n",
    "    \n",
    "    print(f\"{'-' * (len(name) + 4)}\\n| {name} |\\n{'-' * (len(name) + 4)}\")\n",
    "    print(prof.key_averages().table(sort_by=\"device_time_total\", row_limit=20, max_name_column_width=50, top_level_events_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
